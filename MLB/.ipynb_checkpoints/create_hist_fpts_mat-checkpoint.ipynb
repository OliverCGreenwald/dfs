{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/Users/alandu/Documents/DFS/\")\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create first/base case hist_fpts_mat_update matrix (only need to run once at beginning of season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first date\n",
    "date = \"2017-04-02\"\n",
    "\n",
    "results_mat = pd.read_csv(\"MLB/data_warehouse/\" + date + \"/player_results.csv\")\n",
    "results_mat['Name_Team'] = results_mat['Player'] + \"_\" + results_mat['Team']\n",
    "results_mat = results_mat[['Name_Team', 'Actual Score']]\n",
    "results_mat.rename(columns={'Actual Score':date}, inplace = True)\n",
    "results_mat\n",
    "\n",
    "results_mat.to_csv(\"MLB/data_warehouse/\" + \"2017-04-03\" + \"/hist_fpts_mat_update.csv\", index=False, na_rep=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hist_fpts_mat_update for rest of season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-10 00:00:00\n",
      "2017-07-08\n",
      "2017-07-14 00:00:00\n",
      "2017-07-10\n",
      "2017-07-11\n",
      "2017-07-12\n",
      "2017-07-17 00:00:00\n",
      "2017-07-15\n",
      "2017-08-09 00:00:00\n",
      "2017-08-07\n"
     ]
    }
   ],
   "source": [
    "# rest of dates\n",
    "date_list = [datetime.strptime(\"2017-04-04\", '%Y-%m-%d') + timedelta(days=x) for x in range(0, 130)]\n",
    "\n",
    "for date in date_list:\n",
    "    file_path_date = Path(\"MLB/data_warehouse/\" + date.strftime('%Y-%m-%d'))\n",
    "    if file_path_date.exists():\n",
    "        date_tm1 = date - timedelta(days=1)\n",
    "        \n",
    "        # read in hist_fpts_mat_update from date t-1\n",
    "        file_path_hist = Path(\"MLB/data_warehouse/\" + date_tm1.strftime('%Y-%m-%d') + \"/hist_fpts_mat_update.csv\")\n",
    "        if file_path_hist.exists():\n",
    "            hist_fpts_mat = pd.read_csv(str(file_path_hist))\n",
    "        else:\n",
    "            # fill in dates where folder is missing\n",
    "            temp_date = date_tm1 - timedelta(days=1)\n",
    "            file_path_hist = Path(\"MLB/data_warehouse/\" + temp_date.strftime('%Y-%m-%d') + \"/hist_fpts_mat_update.csv\")\n",
    "            num_missing = 1\n",
    "            while (file_path_hist.exists() == False):\n",
    "                temp_date = temp_date - timedelta(days=1)\n",
    "                file_path_hist = Path(\"MLB/data_warehouse/\" + temp_date.strftime('%Y-%m-%d') + \"/hist_fpts_mat_update.csv\")\n",
    "                num_missing += 1\n",
    "            hist_fpts_mat = pd.read_csv(str(file_path_hist))\n",
    "            print(date)\n",
    "            for i in range(0,num_missing):\n",
    "                new_column = temp_date + timedelta(days=i+1)\n",
    "                \n",
    "                # add missing date\n",
    "                new_column_m1 = new_column - timedelta(days=1)\n",
    "                print(new_column_m1.strftime('%Y-%m-%d'))\n",
    "                file_path_results = Path(\"MLB/data_warehouse/\" + new_column_m1.strftime('%Y-%m-%d') + \"/player_results.csv\")\n",
    "                if file_path_results.exists():\n",
    "                    results_mat = pd.read_csv(str(file_path_results))\n",
    "                    results_mat['Name_Team'] = results_mat['Player'] + \"_\" + results_mat['Team']\n",
    "                    hist_fpts_mat = hist_fpts_mat.merge(results_mat[['Name_Team', 'Actual Score']], on=\"Name_Team\", how='outer')\n",
    "                    hist_fpts_mat.rename(columns={'Actual Score':new_column_m1.strftime('%Y-%m-%d')}, inplace = True)\n",
    "                elif Path(\"MLB/data_warehouse/\" + new_column_m1.strftime('%Y-%m-%d')).exists():\n",
    "                    hist_fpts_mat[new_column_m1.strftime('%Y-%m-%d')] = float('NaN')\n",
    "                \n",
    "                # add previous dates NA\n",
    "                hist_fpts_mat[new_column.strftime('%Y-%m-%d')] = float('NaN')\n",
    "        \n",
    "        # add fpts for date t\n",
    "        file_path_results = Path(\"MLB/data_warehouse/\" + date_tm1.strftime('%Y-%m-%d') + \"/player_results.csv\")\n",
    "        if file_path_results.exists():\n",
    "            results_mat = pd.read_csv(str(file_path_results))\n",
    "            results_mat['Name_Team'] = results_mat['Player'] + \"_\" + results_mat['Team']\n",
    "\n",
    "            new_hist_fpts_mat = hist_fpts_mat.merge(results_mat[['Name_Team', 'Actual Score']], on=\"Name_Team\", how='outer')\n",
    "            new_hist_fpts_mat.rename(columns={'Actual Score':date_tm1.strftime('%Y-%m-%d')}, inplace = True)\n",
    "        else:\n",
    "            new_hist_fpts_mat = hist_fpts_mat\n",
    "            new_hist_fpts_mat[date_tm1.strftime('%Y-%m-%d')] = float('NaN')\n",
    "        \n",
    "        # write to file\n",
    "        new_hist_fpts_mat.to_csv(str(file_path_date) + \"/hist_fpts_mat_update.csv\", index=False, na_rep=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create full covariance matrix for entire season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_list = [datetime.strptime(\"2017-04-04\", '%Y-%m-%d') + timedelta(days=x) for x in range(0, 130)]\n",
    "for date in date_list:\n",
    "    # create covariance matrix\n",
    "    file_path_hist = Path(\"MLB/data_warehouse/\" + date.strftime('%Y-%m-%d') + \"/hist_fpts_mat_update.csv\")\n",
    "    if file_path_hist.exists():\n",
    "        hist_fpts_mat_update = pd.read_csv(str(file_path_hist), header=None)\n",
    "        hist_fpts_mat_update = hist_fpts_mat_update[1:] # remove first row (header)\n",
    "\n",
    "        player_team_vec = hist_fpts_mat_update[0]\n",
    "        player_team_vec = player_team_vec.reset_index(drop=True)\n",
    "\n",
    "        hist_fpts_mat_update = hist_fpts_mat_update.drop(0, 1) # remove first column (names)\n",
    "        hist_fpts_mat_update = hist_fpts_mat_update.astype(float)\n",
    "        cov_mat_masked = np.ma.array(hist_fpts_mat_update, mask=np.isnan(hist_fpts_mat_update))\n",
    "        cov_mat = np.ma.cov(cov_mat_masked, allow_masked=True)\n",
    "        cov_mat = np.ma.getdata(cov_mat)\n",
    "        \n",
    "        # get list of team names in order\n",
    "        column_teams = []\n",
    "        for player_team in player_team_vec:\n",
    "            column_teams.append(player_team.split('_')[1])\n",
    "\n",
    "        # set covariance to 0 if not on same team\n",
    "        for ind_row, player_team in enumerate(player_team_vec):\n",
    "            row_team = player_team_vec[ind_row].split('_')[1]\n",
    "            for ind_col, column_team in enumerate(column_teams):\n",
    "                if row_team != column_team:\n",
    "                    cov_mat[ind_row, ind_col] = 0.0\n",
    "                    \n",
    "        # TODO: set to 0 if less than played in less than 10% of games\n",
    "                    \n",
    "        # convert to pandas df to add column names\n",
    "        cov_mat = pd.DataFrame(cov_mat)\n",
    "        cov_mat.columns = player_team_vec\n",
    "        \n",
    "        # write to file\n",
    "        cov_mat.to_csv(\"MLB/data_warehouse/\" + date.strftime('%Y-%m-%d') + \"/covariance_mat_update.csv\", index=False, na_rep=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset covariance matrix for each contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path_hist = Path(\"MLB/data_warehouse/\" + \"2017-08-05\" + \"/hist_fpts_mat_update.csv\")\n",
    "file_path_date = Path(\"MLB/data_warehouse/\" + \"2017-08-05\")\n",
    "if file_path_hist.exists():\n",
    "    # read in hist_fpts_mat_update\n",
    "    hist_fpts_mat_update = pd.read_csv(str(file_path_hist), header=None)\n",
    "    \n",
    "    # read in hitters.csv\n",
    "    hitters_df = pd.read_csv(str(file_path_date) + \"/$40.00entry_MLB$250KSaturdaySlugfest[$50Kto1st]/hitters.csv\")\n",
    "    \n",
    "#     # find indicies of each player in hitters_df in hist_fpts_mat_update\n",
    "#     hitters_name_team = hitters_df['Name'] + \"_\" + hitters_df['teamAbbrev']\n",
    "#     inds_match = []\n",
    "#     for name_team in hitters_name_team:\n",
    "#         inds_match.append(hist_fpts_mat_update[hist_fpts_mat_update[0] == name_team].index.tolist())\n",
    "#     inds_match = sum(inds_match, []) # unlist\n",
    "    \n",
    "#     # read in covariance_mat_update\n",
    "#     covariance_mat_update = pd.read_csv(str(file_path_date) + \"/covariance_mat_update.csv\")\n",
    "#     covariance_mat_update = np.array(covariance_mat_update)\n",
    "#     covariance_mat_update = covariance_mat_update[np.ix_(inds_match,inds_match)]\n",
    "#     covariance_mat_update = pd.DataFrame(covariance_mat_update)\n",
    "#     covariance_mat_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_mat_update.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitters_name_team.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find indicies of each player in hitters_df in hist_fpts_mat_update\n",
    "hitters_name_team = hitters_df['Name'] + \"_\" + hitters_df['teamAbbrev']\n",
    "inds_match = []\n",
    "for name_team in hitters_name_team:\n",
    "    inds_match.append(hist_fpts_mat_update[hist_fpts_mat_update[0] == name_team].index.tolist())\n",
    "inds_found = sum(inds_match, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138], [86], [85], [82], [9], [79], [92], [10], [148], [94], [96], [117], [119], [105], [831], [248], [100], [36], [145], [850], [128], [150], [226], [164], [97], [161], [188], [804], [110], [152], [83], [446], [142], [15], [344], [87], [604], [125], [159], [104], [661], [788], [111], [174], [124], [274], [120], [172], [171], [260], [625], [], [357], [129], [631], [24], [], [27], [141], [], [121], [405], [480], [702], [134], [113], [232], [779], [342], [29], [565], [112], [162], [137], [43], [22], [202], [856], [14], [95], [103], [801], [795], [268], [346], [193], [703], [98], [132], [154], [355], [175], [362], [204], [774], [], [136], [169], [725], [427], [784], [199], [545], [278], [817], [88], [300], [178], [99], [592], [197], [843], [], [115], [108], [], [], [], [91], [601], [351], [345], [492], [116], [228], [200], [308], [682], [257], [127], [698], [741], [225], [390], [265], [17], [126], [663], [249], [252], [307], [474], [167], [], [109], [122], [836], [212], [181], [783], [107], [745], [455], [275], [245], [38], [153], [301], [44], [189], [234], [101], [192], [347], [211], [], [280], [], [183], [510], [309], [147], [238], [139], [241], [403], [239], [641], [11], [48], [755], [33], [], [808], [52], [574], [451], [143], [], [], [805], [566], [209], [251], [253], [157], [], [], [546], [13], [], [525], [198], [25], [235], [550], [201], [579], [485], [589], [508], [156], [583], [243], [304], [229], [270], [218], [31], [155], [414], [413], [], [216], [254], [435], [], [55], [623], [177], [191], [217], [269], [259], [227], [231], [758], [785], [255], [388], [37], [240], [769], [587], [151], [246], [417], [772], [], [722], [791], [556], [176], [475], [818], [39], [131], [], [], [], [222], [489], [242], [205], [], [798], [401], [497], [449], [491], [102], [], [250], [], [233], [], [261], [478], [], [787], [476], [354], [368], [356], [237], [], [343], [672], [635], [849], [704], [230], [258], [272], [23], [93], [494], [521], [433], [21], [678], [], [262], [636], [206], [264], [], [585], [], [207], [180], [411], [166], [], [], [509], [624], [], [531], [588], [42], [20], [851], [], [46], [353], [256], [752], [823], [168], [306], [539], [634], [], [696], [633], [590], [701], [436], [], [674], [273], [194], [638], [], [], [792], [], [822], [30], [593], [311], [658], [56], [737], [190], [], [], [310], [], [], [858], [543], [365], [690], [644], [387], [281], [373], [], [519], [829], [], [], [], [670], [], [671], [652], [381], [], [], [349], [611], [], [], [613], [], [], [477], [520], [382], [437], [673], [857], [595], [685], [488], [236], [], [277], [532], [], [], [612], [764], [317], [282], [688], [602], [], [], [598], [677], [544], [806], [], [662], [507], [385], [684], [575], [522], [], [392], [447], [771], [726], [372], [746], [512], [], [], [523], [279], [483], [639], [], [223], [647], [479], [], [705], [516], [371], [571], [619], [609], [809], [646], [799], [224], [], [484], [384], [594], [686], [824], [860], [599], [], [], [], [], [], [], [505], [733], [], [528], [830], [498], [], [717], [819], [711], [859], [552], [796], [732], [731], [576], [], [412], [660], [], [], [313], [], [], [], [854], [716], [773], [713], [777], [807], [845]]\n"
     ]
    }
   ],
   "source": [
    "print(inds_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# find indicies of missing players (will be 0's in covariance matrix)\n",
    "inds_missing_1 = []\n",
    "inds_missing_2 = []\n",
    "for i, ind in enumerate(inds_match):\n",
    "    if (len(ind) == 0) & (i < len(inds_found)):\n",
    "        inds_missing_1.append(i)\n",
    "    elif (len(ind) == 0) & (i >= len(inds_found)):\n",
    "        inds_missing_2.append(i)\n",
    "print(len(inds_missing_1))\n",
    "print(len(inds_missing_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, ind in enumerate(inds_missing_1):\n",
    "    inds_missing_1[i] += i\n",
    "    \n",
    "for i, ind in enumerate(inds_missing_2):\n",
    "    inds_missing_2[i] += i + len(inds_missing_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 57, 61, 98, 116, 120, 122, 124, 151, 174, 177, 193, 200, 202, 210, 212, 216, 239, 244, 267, 277, 279, 281, 287, 295, 298, 301, 305, 313, 330, 336, 339, 345, 347, 351, 358, 369, 376, 382, 384, 387, 397, 399, 402, 404, 414, 418, 420, 422, 425, 430, 432, 436, 438, 441, 443, 455, 459, 461, 469, 471]\n",
      "[477, 485, 494, 496, 502, 507, 519, 528, 530, 532, 534, 536, 538, 542, 547, 558, 562, 564, 567, 569, 571]\n"
     ]
    }
   ],
   "source": [
    "print(inds_missing_1)\n",
    "print(inds_missing_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in covariance_mat_update\n",
    "covariance_mat_update = pd.read_csv(str(file_path_date) + \"/covariance_mat_update.csv\")\n",
    "covariance_mat_update = np.array(covariance_mat_update)\n",
    "covariance_mat_update = covariance_mat_update[np.ix_(inds_found,inds_found)]\n",
    "covariance_mat_update = pd.DataFrame(covariance_mat_update)\n",
    "covariance_mat_update.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add 0's in missing_inds row and column\n",
    "# covariance_mat_update.loc[2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 14)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitters_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_mat_update.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 477)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.array(covariance_mat_update)\n",
    "for ind in inds_missing_1:\n",
    "    temp = np.insert(temp, ind, 0, axis=1)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477, 477)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ind in inds_missing_1:\n",
    "    temp = np.insert(temp, ind, 0, axis=0)\n",
    "temp =  pd.DataFrame(temp)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477, 477)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf = pd.DataFrame(temp)\n",
    "asdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asdf.loc[len(asdf)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 477)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
